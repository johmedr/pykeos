<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.1" />
<title>pykeos.tools.corr_utils API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>pykeos.tools.corr_utils</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import numpy as np
from ..tools import n_ball_volume, n_sphere_area, delay_coordinates, lstsqr
from .math_utils import _lstsqr_design_matrix
from scipy.special import gamma
from nolds.measures import poly_fit
from tqdm import tqdm
from typing import Union

import plotly.graph_objs as go


def _fast_count_row_1d(x, traj, r, norm_p):
    return np.count_nonzero(traj - x[np.newaxis, :] &lt;= r)


def _fast_count_row(x, traj, r, norm_p):
    &#34;&#34;&#34;
    :param x: (dim, )
    :param traj: (n_points, dim)
    :param norm_p: int, float(&#39;inf&#39;)
    :return:
    &#34;&#34;&#34;
    return np.count_nonzero(np.linalg.norm(traj - x[np.newaxis, :], ord=norm_p, axis=1) &lt;= r)


def _fast_count_traj(x, r, norm_p):
    &#34;&#34;&#34;
    :param x: (n_points, dim)
    :param norm_p: int, float(&#39;inf&#39;)
    :return:
    &#34;&#34;&#34;
    if x.shape[1] &gt; 1:
        return np.sum(np.apply_along_axis(_fast_count_row, 1, x, x, r, norm_p))
    elif x.shape[1] == 1:
        return np.sum(np.apply_along_axis(_fast_count_row_1d, 1, x, x, r, norm_p))


def corr_sum(traj, r, norm_p=1, allow_equals=False):
    if allow_equals:
        return _fast_count_traj(traj, r, norm_p).astype(np.float64) / traj.shape[0] ** 2
    else:
        return (_fast_count_traj(traj, r, norm_p).astype(np.float64) - traj.shape[0]) /\
               (traj.shape[0] * (traj.shape[0] - 1))


def dsty_est(x, samples, r, norm_p=1):
    if len(samples.shape) == 1:
        samples = samples[:, np.newaxis]

    dim = samples.shape[1]

    # x is (k,)
    if len(x.shape) == 1:
        # if k = d, then x is a point in phase space
        if x.shape[0] == dim:
            x = x[np.newaxis, :]
        # x is a collection of sample points in 1d phase space
        elif dim == 1:
            x = x[:, np.newaxis]
    elif len(x.shape) == 2:
        assert(x.shape[1] == samples.shape[1])

    return np.apply_along_axis(_fast_count_row, 1, x, samples, r, norm_p).astype(np.float64) / (n_ball_volume(dim, norm_p) * r**dim * samples.shape[0])


def reference_rule_alpha(p: Union[float, int], d: int):
    # if d &gt; 1:
    #     return (
    #                    ((d * n_ball_volume(d, p) * (2 * np.sqrt(np.pi))**d) / (d + 2))
    #                  * ((3 * gamma((d + 2) / p + 1))/(2 * gamma((d-1)/p + 1) * gamma(3 / p + 1)))**2
    #            ) ** (1./(d+4))
    # else:
    #     return (12 * np.sqrt(np.pi)) ** (1 / 5.)
    if d == 1:
        return 1.843

    return (
        (4* (2 * np.sqrt(np.pi))**d *(3 * gamma(1+(d+2)/p) * gamma(1+1./p))**2) / ((d + 2) * n_ball_volume(d,p) * (gamma(3/p + 1) * gamma(d/p + 1))**2)
    )**(1/(d+4))


def reference_rule(x: np.ndarray, dim:Union[int, str] = &#39;auto&#39;, norm_p: Union[int, float, str] = 2) -&gt; float:
    n = x.shape[0]
    if dim == &#39;auto&#39;:
        d = 1
        if len(x.shape) == 2:
            d = x.shape[1]

    elif isinstance(dim, int):
        d = dim

    # print(d)
    std = np.sqrt(x.var(axis=0, ddof=1).mean())
    from scipy import stats
    iqr = stats.iqr(x)
    scale = min(std, iqr/1.34)

    gamma_n = n ** (-1/(d+4))

    if norm_p in [&#39;manhattan&#39;, &#39;euclidean&#39;, &#39;supremum&#39;]:
        norm_p = [&#34;manhattan&#34;, &#34;euclidean&#34;].index(norm_p) + 1 if norm_p != &#34;supremum&#34; else float(&#34;inf&#34;)
    alpha_p_d = reference_rule_alpha(norm_p, d)
    return gamma_n * alpha_p_d * scale

# def rule_of_thumb(x: np.ndarray, norm_p=2, version: str = &#39;normal&#39;) -&gt; float:
#     n = x.shape[0]
#     d = 1
#     if len(x.shape) == 2:
#         d = x.shape[1]
#
#     if norm_p in [&#39;manhattan&#39;, &#39;euclidean&#39;, &#39;supremum&#39;]:
#         norm_p = [&#34;manhattan&#34;, &#34;euclidean&#34;].index(norm_p) + 1 if norm_p != &#34;supremum&#34; else float(&#34;inf&#34;)
#
#     std = np.sqrt(x.var(axis=0, ddof=1).mean())
#
#
#     # version 1
#     if version == &#39;normal&#39;:
#         return std * ((9. * n_ball_volume(d, norm_p) * (2 * np.sqrt(np.pi)) ** d)/ ((d + 2) * n)) ** (1/(d+4))
#     elif version == &#39;scott&#39;:
#         return std * 3.5 * n ** (-1 / (d + 2))
#     # version 2
#     # return std * (((d + 2)**2 * (2*np.sqrt(np.pi))**d) / (n * n_ball_volume(d, norm_p) * (1/2. * d + 1/4. * d**2))) ** (1/(d+4))


def grassberger_proccacia(x: np.ndarray, rvals=None, rmin=None, rmax=None, omit_plateau=True, norm_p=2, method=&#39;lstsqr&#39;,
                          hack_filter_rvals=None,  nr=20, plot=False, fig=None, show=True, full_output=False,
                          log_base=10, remove_tail=True, verbose=False):
    &#34;&#34;&#34;
    Estimates the correlation dimension using the Grassberger-Proccacia algorithm. The code is greatly inspired by
    nolds: https://github.com/CSchoel/nolds/blob/master/nolds/measures.py and makes use of nolds version of poly_fit
    :param x: time-series (n_points, dim) or (n_points, )
    :param rvals: the threshold values to use
    :return: the correlation dimension
    &#34;&#34;&#34;
    if log_base == 10:
        log = np.log10
    elif log_base == np.exp(1):
        log = np.log
    else:
        log = lambda x: np.log(x)/np.log(log_base)

    if rvals is None:
        if rmin is not None and rmax is not None:
            rvals = np.logspace(rmin, rmax, nr, base=log_base)
        else:
            rvals = np.logspace(- log(x.shape[0]) + log(x.std()) - 2, 5 + log(x.std()), nr, base=log_base)
    # print(rvals)
    csums = np.asarray([corr_sum(x, r, norm_p=norm_p)
                        for r in (tqdm(rvals, desc=&#34;Computing correlation sums&#34;) if verbose else rvals)])
    # print(csums)
    orig_log_csums =log(csums[csums &gt; 0])
    orig_log_rvals = log(rvals[csums &gt; 0])

    log_csums = np.asarray(orig_log_csums)
    log_rvals = np.asarray(orig_log_rvals)


    if remove_tail:
        filter = log_csums &gt; -log(x.shape[0])
        log_csums = log_csums[filter]
        log_rvals = log_rvals[filter]

    if hack_filter_rvals is not None:
        log_rvals = log_rvals[hack_filter_rvals]
        log_csums = log_csums[hack_filter_rvals]

    if omit_plateau:
        filter = np.zeros_like(log_rvals, dtype=np.bool)
        for i in range(log_rvals.size - 1):
            delta = log_csums[i+1] - log_csums[i]
            if delta &gt; 0:
                filter[i] = True

        log_rvals = log_rvals[filter]
        log_csums = log_csums[filter]

    # poly = poly_fit(log_rvals, log_csums, degree=1)
    if len(log_rvals) == 0:
        raise ValueError(&#39;bad estim&#39;)

    if method == &#39;lstsqr&#39;:
        poly = lstsqr(_lstsqr_design_matrix(log_rvals), log_csums)
    else:
        poly = np.polyfit(log_rvals, log_csums, deg=1)

    if plot:
        if fig is None:
            fig = go.Figure()
        fig.add_trace(go.Scatter(x=orig_log_rvals, y=orig_log_csums, name=&#34;log(C(r)) vs log(r)&#34;))
        fig.add_trace(go.Scatter(x=orig_log_rvals, y=poly[1] + orig_log_rvals * poly[0],
                                 name=&#34;%.2f log(r) + %.2f&#34;%(poly[0], poly[1]), line=dict(dash=&#34;dash&#34;),
                                 marker=dict(symbol=&#34;x-thin&#34;)))

        if show:
            fig.show()

    if full_output:
        return [orig_log_rvals, orig_log_csums, poly]
    else:
        return poly[0]

def approximate_d2(x: object, r_opt: object = None, meaningfull_range: object = (0.5, 1.), n_evals: object = 10, base: object = 10.,
                   norm_p: float = float(&#39;inf&#39;),
                   method: object = &#39;fit&#39;,
                   full_output: object = False, output_curve = False) -&gt; float:
    if r_opt is None:
        r_opt = reference_rule(x, norm_p=norm_p)
    if base ==10:
        log = np.log10
    elif base == np.e:
        log = np.log
    else:
        log = lambda x: np.log(x) / np.log(base)

    rvals = np.logspace(log(r_opt * meaningfull_range[0]), log(r_opt*meaningfull_range[1]), n_evals, base=base)
    _csum =lambda r : corr_sum(x, r, norm_p=norm_p)
    csums = np.vectorize(_csum)(rvals)
    csums = log(csums)
    csums = csums[csums == csums]
    rvals = log(rvals)

    if method == &#39;tangent&#39;:
        slopes = []
        for i in range(1, len(rvals)):
            rsup = rvals[i]
            for j in range(i):
                rinf = rvals[j]
                slopes.append((csums[i] - csums[j])/(rsup - rinf))

        return np.mean(slopes)
    elif method == &#39;fit&#39;:
        poly = lstsqr(_lstsqr_design_matrix(rvals), csums)
        # poly = poly_fit(rvals, csums, degree=1)
        if full_output:
            if output_curve:
                return poly[0], poly[1],  log(r_opt), rvals, csums
            else:
                return poly[0], poly[1],  log(r_opt)
        else:
            return poly[0]



def corrdim_tangent_approx(x: np.ndarray, r_opt: float = None, norm_p=1, r_opt_ratio: float = 0.1, base: float = 10.0, full_output=False):

    if len(x.shape) == 1:
        x = x[:, np.newaxis]

    dim = x.shape[1]
    n_points = x.shape[0]

    if r_opt is None:
        r_opt = reference_rule(x, norm_p=norm_p)

    if base == 10:
        log = np.log10
    elif base == np.e:
        log = np.log
    else:
        raise AttributeError()

    r1 = r_opt
    r2 = r_opt * base**(-r_opt_ratio)
    # print(log(r1))
    # print(log(r2))
    c1 = corr_sum(x,  r1, norm_p=norm_p, allow_equals=False)
    c2 = corr_sum(x, r2, norm_p=norm_p, allow_equals=False)
    alpha = (log(c1) - log(c2)) / r_opt_ratio
    if full_output:
        r0 = log(r1)
        beta = log(c1) - alpha * r0

        return alpha, beta, r0
    else:
        return alpha


def approximate_k2(x: np.ndarray=None, r=&#39;auto&#39;, L_min=None, L_max=None, min_diag_number=0,
                   min_consecutive_nonzero_values=5, split_nonzero_slices=True, take_zero_splitted_slice=&#39;all&#39;, dt=1, method=&#39;avg&#39;, rp=None,
                   full_output=False, raise_if_empty=True, ordering=True):
# def approximate_k2(x: np.ndarray, r=&#39;auto&#39;, L_min=None, max_l=15, full_output=False, mrange: tuple=None, dt=1):
    assert(take_zero_splitted_slice in {&#39;first&#39;, &#39;all&#39;})
    assert(method in {&#39;avg&#39;, &#39;fit&#39;})

    try:
        from pyunicorn.timeseries import RecurrencePlot
    except ImportError:
        raise ImportError(&#39;The pyunicorn module is required to approximate K2. &#39;)

    if r == &#39;auto&#39;:
        r = reference_rule(x, norm_p=float(&#39;inf&#39;))
    # if max_l is None:
    #     max_l = -1

    if rp is None:
        if x is None:
            raise AttributeError(&#34;Either x or a pyunicorn RecurrencePlot must be supplied&#34;)
        rp = RecurrencePlot(x, threshold=r, silence_level=3, metric=&#34;supremum&#34;)

    if L_max is None:
        L_max = rp.max_diaglength()
    else:
        L_max = min(L_max, rp.max_diaglength())

    if L_min is not None:
        if isinstance(L_min, str):
            L_min = int(float(L_min.split()[0]) * rp.average_diaglength())

        L_min = min(L_min, 2)
    else:
        L_min = 2


    import itertools
    import operator
    # Entry i corresponds to diagline of size i
    diagline_dist = rp.diagline_dist()[L_min:L_max]
    diagline_dist_slices = []

    if split_nonzero_slices:
        nonzero_diagline_idx = [[i for i,value in it]
                                for key,it in itertools.groupby(enumerate(diagline_dist &gt; min_diag_number), key=operator.itemgetter(1))
                                if key != 0]
        if take_zero_splitted_slice == &#39;all&#39;:
            diagline_dist_slices = [diagline_dist[s] for s in nonzero_diagline_idx
                                    if len(s) &gt; min_consecutive_nonzero_values]
        elif take_zero_splitted_slice == &#34;first&#34;:
            for s in nonzero_diagline_idx:
                if len(s) &gt; min_consecutive_nonzero_values:
                    diagline_dist_slices = [diagline_dist[s]]
                    break

    else:
        diagline_dist_slices = [diagline_dist]



    if len(diagline_dist_slices) == 0:
        if raise_if_empty:
            raise ValueError(&#39;Cannot find sufficient support to estimate K2. Please check the timeseries for NaN or inf &#39;
                             &#39;values or manually provide a valid recurrence plot.&#39;)
        else:
            return np.nan

    k2_list = []

    if method == &#39;fit&#39;:
        x_vals = []
        y_vals = []
        for N_l in diagline_dist_slices:
            # bad_vals_filter = N_l &gt; min_diags
            # bad_vals_filter[1:] = np.logical_and(bad_vals_filter[1:], N_l[:-1] - N_l[1:] != 0)
            # bad_vals_filter = N_l[:-1] - N_l[1:] != 0
            x_vals.append(np.arange(N_l.shape[0])[::-1])
            # x_vals = x_vals[bad_vals_filter]

            # N_l = N_l[bad_vals_filter]
            y_vals.append(np.log(N_l))
            # N_l = np.log(N_l)
        x_vals = np.concatenate(x_vals)
        y_vals = np.concatenate(y_vals)

        poly = lstsqr(_lstsqr_design_matrix(x_vals), y_vals)
        k2_list.append(poly[0])

    elif method == &#39;avg&#39;:
        # D_l = np.zeros((np.sum((len(s) for s in diagline_dist_slices)) - 1, ), dtype=float)

        for N_l in diagline_dist_slices:
            # D_l = np.zeros((N_l.shape[0]-1,), dtype=float)
            D_l = []
            for i in range(N_l.shape[0] - 1):
                if N_l[i + 1] != 0 and N_l[i] != 0:
                    if ordering:
                        # li = np.log(N_l[i])
                        # lip1 = np.log(N_l[i+1])

                        # D_l[i] = li - lip1
                        # D_l.append(li - lip1)
                        if  N_l[i+1] &lt;= N_l[i]:
                            D_l.append(np.log(N_l[i] / N_l[i+1]))
                    else:
                        D_l.append(np.log(N_l[i] / N_l[i+1]))
            # D_l = np.nan_to_num(D_l)
            # D_l = D_l[D_l == D_l]

            k2_list.append(np.mean(D_l))

    if full_output:
        return [k2 / dt for k2 in k2_list]
    else:
        return np.mean(k2_list) / dt</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="pykeos.tools.corr_utils.approximate_d2"><code class="name flex">
<span>def <span class="ident">approximate_d2</span></span>(<span>x: object, r_opt: object = None, meaningfull_range: object = (0.5, 1.0), n_evals: object = 10, base: object = 10.0, norm_p: float = inf, method: object = 'fit', full_output: object = False, output_curve=False) ‑> float</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def approximate_d2(x: object, r_opt: object = None, meaningfull_range: object = (0.5, 1.), n_evals: object = 10, base: object = 10.,
                   norm_p: float = float(&#39;inf&#39;),
                   method: object = &#39;fit&#39;,
                   full_output: object = False, output_curve = False) -&gt; float:
    if r_opt is None:
        r_opt = reference_rule(x, norm_p=norm_p)
    if base ==10:
        log = np.log10
    elif base == np.e:
        log = np.log
    else:
        log = lambda x: np.log(x) / np.log(base)

    rvals = np.logspace(log(r_opt * meaningfull_range[0]), log(r_opt*meaningfull_range[1]), n_evals, base=base)
    _csum =lambda r : corr_sum(x, r, norm_p=norm_p)
    csums = np.vectorize(_csum)(rvals)
    csums = log(csums)
    csums = csums[csums == csums]
    rvals = log(rvals)

    if method == &#39;tangent&#39;:
        slopes = []
        for i in range(1, len(rvals)):
            rsup = rvals[i]
            for j in range(i):
                rinf = rvals[j]
                slopes.append((csums[i] - csums[j])/(rsup - rinf))

        return np.mean(slopes)
    elif method == &#39;fit&#39;:
        poly = lstsqr(_lstsqr_design_matrix(rvals), csums)
        # poly = poly_fit(rvals, csums, degree=1)
        if full_output:
            if output_curve:
                return poly[0], poly[1],  log(r_opt), rvals, csums
            else:
                return poly[0], poly[1],  log(r_opt)
        else:
            return poly[0]</code></pre>
</details>
</dd>
<dt id="pykeos.tools.corr_utils.approximate_k2"><code class="name flex">
<span>def <span class="ident">approximate_k2</span></span>(<span>x: numpy.ndarray = None, r='auto', L_min=None, L_max=None, min_diag_number=0, min_consecutive_nonzero_values=5, split_nonzero_slices=True, take_zero_splitted_slice='all', dt=1, method='avg', rp=None, full_output=False, raise_if_empty=True, ordering=True)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def approximate_k2(x: np.ndarray=None, r=&#39;auto&#39;, L_min=None, L_max=None, min_diag_number=0,
                   min_consecutive_nonzero_values=5, split_nonzero_slices=True, take_zero_splitted_slice=&#39;all&#39;, dt=1, method=&#39;avg&#39;, rp=None,
                   full_output=False, raise_if_empty=True, ordering=True):
# def approximate_k2(x: np.ndarray, r=&#39;auto&#39;, L_min=None, max_l=15, full_output=False, mrange: tuple=None, dt=1):
    assert(take_zero_splitted_slice in {&#39;first&#39;, &#39;all&#39;})
    assert(method in {&#39;avg&#39;, &#39;fit&#39;})

    try:
        from pyunicorn.timeseries import RecurrencePlot
    except ImportError:
        raise ImportError(&#39;The pyunicorn module is required to approximate K2. &#39;)

    if r == &#39;auto&#39;:
        r = reference_rule(x, norm_p=float(&#39;inf&#39;))
    # if max_l is None:
    #     max_l = -1

    if rp is None:
        if x is None:
            raise AttributeError(&#34;Either x or a pyunicorn RecurrencePlot must be supplied&#34;)
        rp = RecurrencePlot(x, threshold=r, silence_level=3, metric=&#34;supremum&#34;)

    if L_max is None:
        L_max = rp.max_diaglength()
    else:
        L_max = min(L_max, rp.max_diaglength())

    if L_min is not None:
        if isinstance(L_min, str):
            L_min = int(float(L_min.split()[0]) * rp.average_diaglength())

        L_min = min(L_min, 2)
    else:
        L_min = 2


    import itertools
    import operator
    # Entry i corresponds to diagline of size i
    diagline_dist = rp.diagline_dist()[L_min:L_max]
    diagline_dist_slices = []

    if split_nonzero_slices:
        nonzero_diagline_idx = [[i for i,value in it]
                                for key,it in itertools.groupby(enumerate(diagline_dist &gt; min_diag_number), key=operator.itemgetter(1))
                                if key != 0]
        if take_zero_splitted_slice == &#39;all&#39;:
            diagline_dist_slices = [diagline_dist[s] for s in nonzero_diagline_idx
                                    if len(s) &gt; min_consecutive_nonzero_values]
        elif take_zero_splitted_slice == &#34;first&#34;:
            for s in nonzero_diagline_idx:
                if len(s) &gt; min_consecutive_nonzero_values:
                    diagline_dist_slices = [diagline_dist[s]]
                    break

    else:
        diagline_dist_slices = [diagline_dist]



    if len(diagline_dist_slices) == 0:
        if raise_if_empty:
            raise ValueError(&#39;Cannot find sufficient support to estimate K2. Please check the timeseries for NaN or inf &#39;
                             &#39;values or manually provide a valid recurrence plot.&#39;)
        else:
            return np.nan

    k2_list = []

    if method == &#39;fit&#39;:
        x_vals = []
        y_vals = []
        for N_l in diagline_dist_slices:
            # bad_vals_filter = N_l &gt; min_diags
            # bad_vals_filter[1:] = np.logical_and(bad_vals_filter[1:], N_l[:-1] - N_l[1:] != 0)
            # bad_vals_filter = N_l[:-1] - N_l[1:] != 0
            x_vals.append(np.arange(N_l.shape[0])[::-1])
            # x_vals = x_vals[bad_vals_filter]

            # N_l = N_l[bad_vals_filter]
            y_vals.append(np.log(N_l))
            # N_l = np.log(N_l)
        x_vals = np.concatenate(x_vals)
        y_vals = np.concatenate(y_vals)

        poly = lstsqr(_lstsqr_design_matrix(x_vals), y_vals)
        k2_list.append(poly[0])

    elif method == &#39;avg&#39;:
        # D_l = np.zeros((np.sum((len(s) for s in diagline_dist_slices)) - 1, ), dtype=float)

        for N_l in diagline_dist_slices:
            # D_l = np.zeros((N_l.shape[0]-1,), dtype=float)
            D_l = []
            for i in range(N_l.shape[0] - 1):
                if N_l[i + 1] != 0 and N_l[i] != 0:
                    if ordering:
                        # li = np.log(N_l[i])
                        # lip1 = np.log(N_l[i+1])

                        # D_l[i] = li - lip1
                        # D_l.append(li - lip1)
                        if  N_l[i+1] &lt;= N_l[i]:
                            D_l.append(np.log(N_l[i] / N_l[i+1]))
                    else:
                        D_l.append(np.log(N_l[i] / N_l[i+1]))
            # D_l = np.nan_to_num(D_l)
            # D_l = D_l[D_l == D_l]

            k2_list.append(np.mean(D_l))

    if full_output:
        return [k2 / dt for k2 in k2_list]
    else:
        return np.mean(k2_list) / dt</code></pre>
</details>
</dd>
<dt id="pykeos.tools.corr_utils.corr_sum"><code class="name flex">
<span>def <span class="ident">corr_sum</span></span>(<span>traj, r, norm_p=1, allow_equals=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def corr_sum(traj, r, norm_p=1, allow_equals=False):
    if allow_equals:
        return _fast_count_traj(traj, r, norm_p).astype(np.float64) / traj.shape[0] ** 2
    else:
        return (_fast_count_traj(traj, r, norm_p).astype(np.float64) - traj.shape[0]) /\
               (traj.shape[0] * (traj.shape[0] - 1))</code></pre>
</details>
</dd>
<dt id="pykeos.tools.corr_utils.corrdim_tangent_approx"><code class="name flex">
<span>def <span class="ident">corrdim_tangent_approx</span></span>(<span>x: numpy.ndarray, r_opt: float = None, norm_p=1, r_opt_ratio: float = 0.1, base: float = 10.0, full_output=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def corrdim_tangent_approx(x: np.ndarray, r_opt: float = None, norm_p=1, r_opt_ratio: float = 0.1, base: float = 10.0, full_output=False):

    if len(x.shape) == 1:
        x = x[:, np.newaxis]

    dim = x.shape[1]
    n_points = x.shape[0]

    if r_opt is None:
        r_opt = reference_rule(x, norm_p=norm_p)

    if base == 10:
        log = np.log10
    elif base == np.e:
        log = np.log
    else:
        raise AttributeError()

    r1 = r_opt
    r2 = r_opt * base**(-r_opt_ratio)
    # print(log(r1))
    # print(log(r2))
    c1 = corr_sum(x,  r1, norm_p=norm_p, allow_equals=False)
    c2 = corr_sum(x, r2, norm_p=norm_p, allow_equals=False)
    alpha = (log(c1) - log(c2)) / r_opt_ratio
    if full_output:
        r0 = log(r1)
        beta = log(c1) - alpha * r0

        return alpha, beta, r0
    else:
        return alpha</code></pre>
</details>
</dd>
<dt id="pykeos.tools.corr_utils.dsty_est"><code class="name flex">
<span>def <span class="ident">dsty_est</span></span>(<span>x, samples, r, norm_p=1)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dsty_est(x, samples, r, norm_p=1):
    if len(samples.shape) == 1:
        samples = samples[:, np.newaxis]

    dim = samples.shape[1]

    # x is (k,)
    if len(x.shape) == 1:
        # if k = d, then x is a point in phase space
        if x.shape[0] == dim:
            x = x[np.newaxis, :]
        # x is a collection of sample points in 1d phase space
        elif dim == 1:
            x = x[:, np.newaxis]
    elif len(x.shape) == 2:
        assert(x.shape[1] == samples.shape[1])

    return np.apply_along_axis(_fast_count_row, 1, x, samples, r, norm_p).astype(np.float64) / (n_ball_volume(dim, norm_p) * r**dim * samples.shape[0])</code></pre>
</details>
</dd>
<dt id="pykeos.tools.corr_utils.grassberger_proccacia"><code class="name flex">
<span>def <span class="ident">grassberger_proccacia</span></span>(<span>x: numpy.ndarray, rvals=None, rmin=None, rmax=None, omit_plateau=True, norm_p=2, method='lstsqr', hack_filter_rvals=None, nr=20, plot=False, fig=None, show=True, full_output=False, log_base=10, remove_tail=True, verbose=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Estimates the correlation dimension using the Grassberger-Proccacia algorithm. The code is greatly inspired by
nolds: <a href="https://github.com/CSchoel/nolds/blob/master/nolds/measures.py">https://github.com/CSchoel/nolds/blob/master/nolds/measures.py</a> and makes use of nolds version of poly_fit
:param x: time-series (n_points, dim) or (n_points, )
:param rvals: the threshold values to use
:return: the correlation dimension</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def grassberger_proccacia(x: np.ndarray, rvals=None, rmin=None, rmax=None, omit_plateau=True, norm_p=2, method=&#39;lstsqr&#39;,
                          hack_filter_rvals=None,  nr=20, plot=False, fig=None, show=True, full_output=False,
                          log_base=10, remove_tail=True, verbose=False):
    &#34;&#34;&#34;
    Estimates the correlation dimension using the Grassberger-Proccacia algorithm. The code is greatly inspired by
    nolds: https://github.com/CSchoel/nolds/blob/master/nolds/measures.py and makes use of nolds version of poly_fit
    :param x: time-series (n_points, dim) or (n_points, )
    :param rvals: the threshold values to use
    :return: the correlation dimension
    &#34;&#34;&#34;
    if log_base == 10:
        log = np.log10
    elif log_base == np.exp(1):
        log = np.log
    else:
        log = lambda x: np.log(x)/np.log(log_base)

    if rvals is None:
        if rmin is not None and rmax is not None:
            rvals = np.logspace(rmin, rmax, nr, base=log_base)
        else:
            rvals = np.logspace(- log(x.shape[0]) + log(x.std()) - 2, 5 + log(x.std()), nr, base=log_base)
    # print(rvals)
    csums = np.asarray([corr_sum(x, r, norm_p=norm_p)
                        for r in (tqdm(rvals, desc=&#34;Computing correlation sums&#34;) if verbose else rvals)])
    # print(csums)
    orig_log_csums =log(csums[csums &gt; 0])
    orig_log_rvals = log(rvals[csums &gt; 0])

    log_csums = np.asarray(orig_log_csums)
    log_rvals = np.asarray(orig_log_rvals)


    if remove_tail:
        filter = log_csums &gt; -log(x.shape[0])
        log_csums = log_csums[filter]
        log_rvals = log_rvals[filter]

    if hack_filter_rvals is not None:
        log_rvals = log_rvals[hack_filter_rvals]
        log_csums = log_csums[hack_filter_rvals]

    if omit_plateau:
        filter = np.zeros_like(log_rvals, dtype=np.bool)
        for i in range(log_rvals.size - 1):
            delta = log_csums[i+1] - log_csums[i]
            if delta &gt; 0:
                filter[i] = True

        log_rvals = log_rvals[filter]
        log_csums = log_csums[filter]

    # poly = poly_fit(log_rvals, log_csums, degree=1)
    if len(log_rvals) == 0:
        raise ValueError(&#39;bad estim&#39;)

    if method == &#39;lstsqr&#39;:
        poly = lstsqr(_lstsqr_design_matrix(log_rvals), log_csums)
    else:
        poly = np.polyfit(log_rvals, log_csums, deg=1)

    if plot:
        if fig is None:
            fig = go.Figure()
        fig.add_trace(go.Scatter(x=orig_log_rvals, y=orig_log_csums, name=&#34;log(C(r)) vs log(r)&#34;))
        fig.add_trace(go.Scatter(x=orig_log_rvals, y=poly[1] + orig_log_rvals * poly[0],
                                 name=&#34;%.2f log(r) + %.2f&#34;%(poly[0], poly[1]), line=dict(dash=&#34;dash&#34;),
                                 marker=dict(symbol=&#34;x-thin&#34;)))

        if show:
            fig.show()

    if full_output:
        return [orig_log_rvals, orig_log_csums, poly]
    else:
        return poly[0]</code></pre>
</details>
</dd>
<dt id="pykeos.tools.corr_utils.reference_rule"><code class="name flex">
<span>def <span class="ident">reference_rule</span></span>(<span>x: numpy.ndarray, dim: Union[int, str] = 'auto', norm_p: Union[int, float, str] = 2) ‑> float</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reference_rule(x: np.ndarray, dim:Union[int, str] = &#39;auto&#39;, norm_p: Union[int, float, str] = 2) -&gt; float:
    n = x.shape[0]
    if dim == &#39;auto&#39;:
        d = 1
        if len(x.shape) == 2:
            d = x.shape[1]

    elif isinstance(dim, int):
        d = dim

    # print(d)
    std = np.sqrt(x.var(axis=0, ddof=1).mean())
    from scipy import stats
    iqr = stats.iqr(x)
    scale = min(std, iqr/1.34)

    gamma_n = n ** (-1/(d+4))

    if norm_p in [&#39;manhattan&#39;, &#39;euclidean&#39;, &#39;supremum&#39;]:
        norm_p = [&#34;manhattan&#34;, &#34;euclidean&#34;].index(norm_p) + 1 if norm_p != &#34;supremum&#34; else float(&#34;inf&#34;)
    alpha_p_d = reference_rule_alpha(norm_p, d)
    return gamma_n * alpha_p_d * scale</code></pre>
</details>
</dd>
<dt id="pykeos.tools.corr_utils.reference_rule_alpha"><code class="name flex">
<span>def <span class="ident">reference_rule_alpha</span></span>(<span>p: Union[float, int], d: int)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reference_rule_alpha(p: Union[float, int], d: int):
    # if d &gt; 1:
    #     return (
    #                    ((d * n_ball_volume(d, p) * (2 * np.sqrt(np.pi))**d) / (d + 2))
    #                  * ((3 * gamma((d + 2) / p + 1))/(2 * gamma((d-1)/p + 1) * gamma(3 / p + 1)))**2
    #            ) ** (1./(d+4))
    # else:
    #     return (12 * np.sqrt(np.pi)) ** (1 / 5.)
    if d == 1:
        return 1.843

    return (
        (4* (2 * np.sqrt(np.pi))**d *(3 * gamma(1+(d+2)/p) * gamma(1+1./p))**2) / ((d + 2) * n_ball_volume(d,p) * (gamma(3/p + 1) * gamma(d/p + 1))**2)
    )**(1/(d+4))</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="pykeos.tools" href="index.html">pykeos.tools</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="pykeos.tools.corr_utils.approximate_d2" href="#pykeos.tools.corr_utils.approximate_d2">approximate_d2</a></code></li>
<li><code><a title="pykeos.tools.corr_utils.approximate_k2" href="#pykeos.tools.corr_utils.approximate_k2">approximate_k2</a></code></li>
<li><code><a title="pykeos.tools.corr_utils.corr_sum" href="#pykeos.tools.corr_utils.corr_sum">corr_sum</a></code></li>
<li><code><a title="pykeos.tools.corr_utils.corrdim_tangent_approx" href="#pykeos.tools.corr_utils.corrdim_tangent_approx">corrdim_tangent_approx</a></code></li>
<li><code><a title="pykeos.tools.corr_utils.dsty_est" href="#pykeos.tools.corr_utils.dsty_est">dsty_est</a></code></li>
<li><code><a title="pykeos.tools.corr_utils.grassberger_proccacia" href="#pykeos.tools.corr_utils.grassberger_proccacia">grassberger_proccacia</a></code></li>
<li><code><a title="pykeos.tools.corr_utils.reference_rule" href="#pykeos.tools.corr_utils.reference_rule">reference_rule</a></code></li>
<li><code><a title="pykeos.tools.corr_utils.reference_rule_alpha" href="#pykeos.tools.corr_utils.reference_rule_alpha">reference_rule_alpha</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.1</a>.</p>
</footer>
</body>
</html>